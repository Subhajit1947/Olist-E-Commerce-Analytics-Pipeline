# âš¡ Olist E-Commerce Analytics Pipeline â€” End-to-End ETL with Spark, Airflow & AWS S3

A production-ready data engineering pipeline that ingests raw Olist e-commerce data from **AWS S3**, processes it using **Apache Spark**, orchestrates workflows with **Apache Airflow**, and stores processed analytics-ready data back into S3.

![Data Pipeline](https://img.shields.io/badge/Data-Pipeline-blue)
![Apache Airflow](https://img.shields.io/badge/Apache-Airflow-orange)
![Apache Spark](https://img.shields.io/badge/Apache-Spark-red)
![Docker](https://img.shields.io/badge/Docker-Containers-blue)
![Python](https://img.shields.io/badge/Python-3.8-green)
![AWS S3](https://img.shields.io/badge/AWS-S3-yellow)

---

## ğŸ—ï¸ System Architecture



![Architecture](app\docs\project-architechture.png)

---

## ğŸ“Œ Overview

- **Source:** Raw CSV files stored in S3  
- **Processing Engine:** Apache Spark (cluster mode)  
- **Orchestration:** Apache Airflow  
- **Storage:** AWS S3 (raw + processed zones)  
- **Containerization:** Docker & Docker Compose  

---

## ğŸ§° Tech Stack

| Component       | Technology          |
|----------------|----------------------|
| Workflow       | Apache Airflow       |
| Processing     | Apache Spark         |
| Storage        | AWS S3               |
| Backend        | PostgreSQL (Airflow) |
| Containers     | Docker               |
| Language       | Python 3.8+          |
| File Format    | Parquet              |

---

# ğŸ“‚ Project Directory Structure

```
olist-E-commerce-etl/
|
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”œâ”€â”€ olist_daily_etl.py
â”‚   â”‚   â”œâ”€â”€ logs/  
â”‚   â”‚   â””â”€â”€ plugins/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”‚   â”œâ”€â”€ olist_orderitems_dataset.csv
â”‚   â”‚   â”œâ”€â”€ olist_orderpayments_dataset.csv
â”‚   â”‚   â”œâ”€â”€ olist_orderreviews_dataset.csv
â”‚   â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â”‚   â”œâ”€â”€ olist_products_dataset.csv
â”‚   â”‚   â”œâ”€â”€ olist_sellers_dataset.csv
â”‚   â”‚   â””â”€â”€ product_category_name_translation.csv
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ business_questions.md
â”‚   â”‚   â”œâ”€â”€ project-architechture.png
â”‚   â”‚   â”œâ”€â”€ setup_guide.md
â”‚   â”‚   â””â”€â”€ star_schema.png
â”‚   â”œâ”€â”€ env
â”‚   â”œâ”€â”€ env.example
â”‚   â”œâ”€â”€ spark_etl/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ spark_config.yaml
â”‚   â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ business_kpis.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ create_dim_date.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ read/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ s3_reader.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ logging_config.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ s3_client_object.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ snowflake_connector.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ spark_session.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â”œâ”€â”€ upload_data_to_s3.py
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements-airflow.txt
â””â”€â”€ requirements.txt
```



---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/Subhajit1947/Olist-E-Commerce-Analytics-Pipeline.git
cd Olist-E-Commerce-Analytics-Pipeline
```

### 2ï¸âƒ£ Create `.env`
```bash
cp .env.example .env
```

Fill AWS keys & S3 paths:
```
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
S3_BUCKET=your_bucket
S3_RAW_DIR=raw
S3_PROCESSED_DIR=processed
```

### 3ï¸âƒ£ Create Airflow Logs Folder
```bash
mkdir app/airflow/logs
```

### 4ï¸âƒ£ Build Containers
```bash
docker-compose build --no-cache
```

### 5ï¸âƒ£ Start Services
```bash
docker-compose up -d
```

### 6ï¸âƒ£ Fix Airflow Spark Connection
In **Airflow Web UI â†’ Admin â†’ Connections â†’ spark_default**

Update host to:
```
spark://spark-master:7077
```

Save.

---

## â–¶ï¸ Running the Pipeline

### **Option 1 â€” Run from Airflow**
- Open DAG: `olist_etl_pipeline`
- Click **Trigger DAG**

### **Option 2 â€” Run from Docker**
```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  /opt/app/spark_etl/main/jobs/business_kpis.py
```

---

## ğŸ“Š Outputs

- Cleaned dimension tables  
- Fact table with revenue & delivery metrics  
- Parquet outputs stored in S3 processed zone  
- Ready for BI reporting  

---

## ğŸ§ª Troubleshooting

### Restart Airflow
```bash
docker-compose down -v
docker-compose up -d
```

### Test S3 Connectivity
```bash
docker exec spark-master python3 -c "import boto3; print(boto3.client('s3').list_buckets())"
```


## ğŸ‘¨â€ğŸ’» Author
Subhajit Das  
Aspiring Data Engineer 

[![GitHub](https://img.shields.io/badge/GitHub-Subhajit1947-black?style=for-the-badge&logo=github)](https://github.com/Subhajit1947)


[![LinkedIn](https://img.shields.io/badge/LinkedIn-Subhajit%20Das-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/subhajit7318/)
  
